# -*- coding: utf-8 -*-
"""NetworkIntrusionDectection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3ug9PTWnzNLpPvPpsFb1cMw4zQFPqiu
"""

import numpy as np
import pandas as pd
from scipy.stats import entropy
from scipy.io import arff
from google.colab import files
from tensorflow.keras.layers import GaussianNoise
from tensorflow.keras.layers import Dense,Input
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import os
from numpy.random import seed
from sklearn.preprocessing import minmax_scale
from sklearn.model_selection import train_test_split
 
from matplotlib import pyplot as plt
# deals with noisy data

from google.colab import drive
drive.mount('/content/drive')

test_set=pd.read_csv('/content/UNSW_NB15_testing-set.csv',header=None)
train_set=pd.read_csv('/content/UNSW_NB15_training-set unlabd - UNSW_NB15_training-set.csv',header=None)

train_set.dropna(inplace=True,axis=1)#drop na's

# The CSV file has no column heads, so add them
train_set.columns = [
    'dur',
    'proto',
    'service',
    'state',
    'spkts',
    'dpkts',
    'sbytes',
    'dbytes',
    'rate',
    'sttl',
    'dttl',
    'sload',
    'dload',
    'sloss',
    'dloss',
    'sinpkt',
    'dinpkt',
    'sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack',
    'ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src',
    'ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login',
    'ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports',
    'attack_cat',
    'label'
]

test_set.columns = [
    'dur',
    'proto',
    'service',
    'state',
    'spkts',
    'dpkts',
    'sbytes',
    'dbytes',
    'rate',
    'sttl',
    'dttl',
    'sload',
    'dload',
    'sloss',
    'dloss',
    'sinpkt',
    'dinpkt',
    'sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack',
    'ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src',
    'ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login',
    'ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports',
    'attack_cat',
    'label'
]


train_set

print('Train data shape', train_set.shape)
print('Test data shape', test_set.shape)

#state
 map = {'ACC':1,'CLO':2,'CON':3,'FIN':4,'INT':5,'REQ':6,'RST':7,'ECO':8,'no':9,'PAR':10,'URN':11}
 #service
 map2 ={'http':1,'ftp':2,'ftp-data':3,'smtp':4,'pop3':5,'dns':6,'snmp':7,'ssl':8,'dhcp':9,'irc':10,'radius':11,'ssh':12}
 #protocol
 map3 = {'udp': 1, 'arp': 2, 'tcp': 3, 'igmp': 4, 'ospf': 5, 'sctp': 6, 'gre': 7, 'ggp': 8, 'ip': 9, 'ipnip': 10, 'st2': 11, 'argus': 12, 'chaos': 13, 'egp': 14, 'emcon': 15, 'nvp': 16, 'pup': 17, 'xnet': 18, 'mux': 19, 'dcn': 20, 'hmp': 21, 'prm': 22, 'trunk-1': 23, 'trunk-2': 24, 'xns-idp': 25, 'leaf-1': 26, 'leaf-2': 27, 'irtp': 28, 'rdp': 29, 'netblt': 30, 'mfe-nsp': 31, 'merit-inp': 32, '3pc': 33, 'idpr': 34, 'ddp': 35, 'idpr-cmtp': 36, 'tp++': 37, 'ipv6': 38, 'sdrp': 39, 'ipv6-frag': 40, 'ipv6-route': 41, 'idrp': 42, 'mhrp': 43, 'i-nlsp': 44, 'rvd': 45, 'mobile': 46, 'narp': 47, 'skip': 48, 'tlsp': 49, 'ipv6-no': 50, 'any': 51, 'ipv6-opts': 52, 'cftp': 53, 'sat-expak': 54, 'ippc': 55, 'kryptolan': 56, 'sat-mon': 57, 'cpnx': 58, 'wsn': 59, 'pvp': 60, 'br-sat-mon': 61, 'sun-nd': 62, 'wb-mon': 63, 'vmtp': 64, 'ttp': 65, 'vines': 66, 'nsfnet-igp': 67, 'dgp': 68, 'eigrp': 69, 'tcf': 70, 'sprite-rpc': 71, 'larp': 72, 'mtp': 73, 'ax.25': 74, 'ipip': 75, 'aes-sp3-d': 76, 'micp': 77, 'encap': 78, 'pri-enc': 79, 'gmtp': 80, 'ifmp': 81, 'pnni': 82, 'qnx': 83, 'scps': 84, 'cbt': 85, 'bbn-rcc': 86, 'igp': 87, 'bna': 88, 'swipe': 89, 'visa': 90, 'ipcv': 91, 'cphb': 92, 'iso-tp4': 93, 'wb-expak': 94, 'sep': 95, 'secure-vmtp': 96, 'xtp': 97, 'il': 98, 'rsvp': 99, 'unas': 100, 'fc': 101, 'iso-ip': 102, 'etherip': 103, 'pim': 104, 'aris': 105, 'a/n': 106, 'ipcomp': 107, 'snp': 108, 'compaq-peer': 109, 'ipx-n-ip': 110, 'pgm': 111, 'vrrp': 112, 'l2tp': 113, 'zero': 114, 'ddx': 115, 'iatp': 116, 'stp': 117, 'srp': 118, 'uti': 119, 'sm': 120, 'smp': 121, 'isis': 122, 'ptp': 123, 'fire': 124, 'crtp': 125, 'crudp': 126, 'sccopmce': 127, 'iplt': 128, 'pipe': 129, 'sps': 130,'ib':131,'icmp':132,'rtp':133}
 #attack_cat
 map4={'Normal': 1, 'Reconnaissance': 2, 'Backdoor': 3, 'DoS': 4, 'Exploits': 5, 'Analysis': 6, 'Fuzzers': 7, 'Worms': 8, 'Shellcode': 9, 'Generic': 10}

 df=pd.DataFrame(train_set) 
 df=df.replace({'-':np.nan})
 df=df.replace(map3)
 df=df.replace(map)
 df=df.replace(map2)
 df=df.replace(map4)

 train_set=df
 train_set.dropna(inplace=True,axis=0)

df=pd.DataFrame(test_set)
df=df.replace(map3)
df=df.replace(map)
df=df.replace(map2)
df=df.replace(map4)
df=df.replace({'-':np.nan})
test_set=df
test_set.dropna(inplace=True,axis=0)

test_set

train_set

target = train_set['label']
test_target = test_set['label']
train_set.drop(['label'], axis=1, inplace=True)
test_set.drop(['label'], axis=1, inplace=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(train_set)
train_set = scaler.transform(train_set)
test_set = scaler.transform(test_set)

pd.DataFrame(train_set)

"""Stack Denoising AutoEncoder"""

#adding noise to the dataset
clean_data = train_set
mu, sigma = 0, 0.1 
# creating a noise with the same dimension as the dataset (2,2) 
noise = np.random.normal(mu, sigma, [35179, 43]) 
print(noise[0:5])

noisy_data = clean_data + noise
print(noisy_data[0:5])
print('train data shape', noisy_data.shape)

pd.DataFrame(noisy_data)

"""Denoising AutoEncoder 1"""

input_dim = noisy_data.shape[1]

feature_dim = [35, 30, 25, 20]
inputs = Input(shape=(input_dim,))
encoded = inputs
encoded = Dense(feature_dim[0], kernel_initializer="uniform")(encoded)
encoded = Dense(feature_dim[1], kernel_initializer="uniform")(encoded)
encoded = Dense(feature_dim[2], kernel_initializer="uniform")(encoded)
encoded = Dense(feature_dim[3], kernel_initializer="uniform")(encoded)

decoded = encoded
decoded = Dense(feature_dim[2], kernel_initializer="uniform")(decoded)
decoded = Dense(feature_dim[1], kernel_initializer="uniform")(decoded)
decoded = Dense(feature_dim[0], kernel_initializer="uniform")(decoded)
decoded = Dense(input_dim, kernel_initializer="uniform")(decoded)

autoencoder = Model(inputs, decoded)
autoencoder.compile(optimizer='adadelta', loss='mse')

autoencoder.summary()

history = autoencoder.fit(noisy_data,train_set,epochs=50 ,shuffle = False).history

plt.plot(history['loss'], linewidth=2, label='Train')
plt.legend(loc='upper right')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.ylim(ymin=0.70,ymax=1)
plt.show()

from keras.models import Sequential

featuremodel = Sequential()
featuremodel.add(Dense(feature_dim[0], input_shape=(input_dim,), weights=autoencoder.layers[1].get_weights()))
featuremodel.add(Dense(feature_dim[1], weights=autoencoder.layers[2].get_weights()))
featuremodel.add(Dense(feature_dim[2], weights=autoencoder.layers[3].get_weights()))
featuremodel.add(Dense(feature_dim[3], weights=autoencoder.layers[4].get_weights()))

featuremodel.compile(optimizer='adadelta', loss='mse')

"""DAE 2"""

clean_data1 = featuremodel.predict(train_set)
mu, sigma = 0, 0.1 
noise1 = np.random.normal(mu, sigma, [35179, 20]) 
print(noise[0:5])

noisy_data1 = clean_data1 + noise1
print(noisy_data1[0:5])
print('train data shape', noisy_data1.shape)

input_dim1 = noisy_data1.shape[1]

feature_dim1 = [18, 14, 10, 6]
inputs1 = Input(shape=(input_dim1,))
encoded1 = inputs1
encoded1 = Dense(feature_dim1[0], kernel_initializer="uniform")(encoded1)
encoded1 = Dense(feature_dim1[1], kernel_initializer="uniform")(encoded1)
encoded1 = Dense(feature_dim1[2], kernel_initializer="uniform")(encoded1)
encoded1 = Dense(feature_dim1[3], kernel_initializer="uniform")(encoded1)

decoded1 = encoded1
decoded1 = Dense(feature_dim1[2], kernel_initializer="uniform")(decoded1)
decoded1 = Dense(feature_dim1[1], kernel_initializer="uniform")(decoded1)
decoded1 = Dense(feature_dim1[0], kernel_initializer="uniform")(decoded1)
decoded1 = Dense(input_dim1, kernel_initializer="uniform")(decoded1)

autoencoder1 = Model(inputs1, decoded1)
autoencoder1.compile(optimizer='adadelta', loss='mse')

autoencoder1.summary()

history1 = autoencoder1.fit(noisy_data1,featuremodel.predict(train_set),epochs=50, shuffle = False).history

plt.plot(history1['loss'], linewidth=2, label='Train')
plt.legend(loc='upper right')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.ylim(ymin=0.70,ymax=1)
plt.show()

featuremode2 = Sequential()
featuremode2.add(Dense(feature_dim1[0], input_shape=(input_dim1,), weights=autoencoder1.layers[1].get_weights()))
featuremode2.add(Dense(feature_dim1[1], weights=autoencoder1.layers[2].get_weights()))
featuremode2.add(Dense(feature_dim1[2], weights=autoencoder1.layers[3].get_weights()))
featuremode2.add(Dense(feature_dim1[3], weights=autoencoder1.layers[4].get_weights()))

featuremode2.compile(optimizer='adadelta', loss='mse')

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

clf = SGDClassifier(loss="hinge", penalty="l2")
clf.fit(featuremode2.predict(featuremodel.predict(train_set)), target)

y_pred = clf.predict(featuremode2.predict(featuremodel.predict(test_set)))

print('Accuracy: {:.2f}'.format(accuracy_score(test_target, y_pred)))

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
from sklearn.metrics import mean_squared_error
plot1 = plt.figure(figsize=(12,6))
plt.plot(y_pred, label='Predicted')
plt.plot(test_target, label='Actual')
plt.legend(prop={'size': 16})
plt.show()
print('Mean Squared Error :',mean_squared_error(test_target, y_pred))